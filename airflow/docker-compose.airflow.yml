# ============================================================================
# NBA Betting Pipeline - Apache Airflow Docker Compose
# ============================================================================
#
# This docker-compose file sets up Apache Airflow for the NBA betting pipeline.
#
# Services:
#   - airflow-webserver: Web UI (port 8080)
#   - airflow-scheduler: DAG scheduler
#   - airflow-worker: Celery worker (optional, for CeleryExecutor)
#   - airflow-init: One-time initialization
#   - postgres: Airflow metadata database
#   - redis: Celery broker (optional, for CeleryExecutor)
#
# Quick Start:
#   docker-compose -f docker-compose.airflow.yml up -d
#
# Access UI:
#   http://localhost:8080 (user: admin, password: admin)
#
# ============================================================================

version: '3.8'

x-airflow-common: &airflow-common
  image: apache/airflow:2.8.1-python3.11
  environment: &airflow-common-env
    # Airflow Core
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__DEFAULT_TIMEZONE: 'America/New_York'

    # Database
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow

    # Webserver
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    AIRFLOW__WEBSERVER__SECRET_KEY: 'nba-betting-pipeline-secret-key-change-me'

    # Scheduler
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: 30

    # API
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'

    # Email (configure for alerts)
    AIRFLOW__EMAIL__EMAIL_BACKEND: 'airflow.utils.email.send_email_smtp'
    AIRFLOW__SMTP__SMTP_HOST: '${SMTP_HOST:-localhost}'
    AIRFLOW__SMTP__SMTP_PORT: '${SMTP_PORT:-25}'
    AIRFLOW__SMTP__SMTP_USER: '${SMTP_USER:-}'
    AIRFLOW__SMTP__SMTP_PASSWORD: '${SMTP_PASSWORD:-}'
    AIRFLOW__SMTP__SMTP_MAIL_FROM: '${SMTP_MAIL_FROM:-airflow@example.com}'
    AIRFLOW__SMTP__SMTP_STARTTLS: 'false'
    AIRFLOW__SMTP__SMTP_SSL: 'false'

    # Project-specific environment variables
    PYTHONPATH: /opt/airflow/project
    NBA_DB_HOST: '${NBA_DB_HOST:-host.docker.internal}'
    NBA_INT_DB_PORT: '${NBA_INT_DB_PORT:-5539}'
    NBA_INT_DB_NAME: '${NBA_INT_DB_NAME:-nba_intelligence}'
    DB_USER: '${DB_USER:-mlb_user}'
    DB_PASSWORD: '${DB_PASSWORD:-${DB_PASSWORD}}'
    ODDS_API_KEY: '${ODDS_API_KEY:-}'

  volumes:
    - ./dags:/opt/airflow/dags
    - ./plugins:/opt/airflow/plugins
    - ./logs:/opt/airflow/logs
    - ../:/opt/airflow/project:ro
    - ../nba/betting_xl/lines:/opt/airflow/project/nba/betting_xl/lines
    - ../nba/betting_xl/predictions:/opt/airflow/project/nba/betting_xl/predictions
    - ../nba/betting_xl/logs:/opt/airflow/project/nba/betting_xl/logs
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on: &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:
  # --------------------------------------------------------------------------
  # Airflow Metadata Database
  # --------------------------------------------------------------------------
  postgres:
    image: postgres:15
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: \${POSTGRES_PASSWORD:-changeme}
      POSTGRES_DB: airflow
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - airflow-network

  # --------------------------------------------------------------------------
  # Redis (Optional - for CeleryExecutor)
  # --------------------------------------------------------------------------
  # Uncomment this section if using CeleryExecutor
  # redis:
  #   image: redis:7
  #   container_name: airflow-redis
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #   restart: unless-stopped
  #   networks:
  #     - airflow-network

  # --------------------------------------------------------------------------
  # Airflow Webserver
  # --------------------------------------------------------------------------
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - airflow-network
      - nba-network

  # --------------------------------------------------------------------------
  # Airflow Scheduler
  # --------------------------------------------------------------------------
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - airflow-network
      - nba-network

  # --------------------------------------------------------------------------
  # Airflow Worker (Optional - for CeleryExecutor)
  # --------------------------------------------------------------------------
  # Uncomment this section if using CeleryExecutor
  # airflow-worker:
  #   <<: *airflow-common
  #   container_name: airflow-worker
  #   command: celery worker
  #   healthcheck:
  #     test:
  #       - "CMD-SHELL"
  #       - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #   environment:
  #     <<: *airflow-common-env
  #     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  #     AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
  #     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
  #   restart: unless-stopped
  #   depends_on:
  #     <<: *airflow-common-depends-on
  #     redis:
  #       condition: service_healthy
  #   networks:
  #     - airflow-network
  #     - nba-network

  # --------------------------------------------------------------------------
  # Airflow Triggerer (for deferrable operators)
  # --------------------------------------------------------------------------
  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - airflow-network
      - nba-network

  # --------------------------------------------------------------------------
  # Airflow Initialization
  # --------------------------------------------------------------------------
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Create directories
        mkdir -p /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins

        # Initialize database
        airflow db init

        # Create admin user
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin

        # Create Airflow Variables
        airflow variables set nba_project_root "/opt/airflow/project"
        airflow variables set alert_email "alerts@example.com"

        # Create Airflow Connections for NBA databases
        airflow connections add 'nba_players_db' \
          --conn-type 'postgres' \
          --conn-host '${NBA_DB_HOST:-host.docker.internal}' \
          --conn-schema 'nba_players' \
          --conn-login '${DB_USER:-mlb_user}' \
          --conn-password '${DB_PASSWORD:-${DB_PASSWORD}}' \
          --conn-port '5536'

        airflow connections add 'nba_games_db' \
          --conn-type 'postgres' \
          --conn-host '${NBA_DB_HOST:-host.docker.internal}' \
          --conn-schema 'nba_games' \
          --conn-login '${DB_USER:-mlb_user}' \
          --conn-password '${DB_PASSWORD:-${DB_PASSWORD}}' \
          --conn-port '5537'

        airflow connections add 'nba_team_db' \
          --conn-type 'postgres' \
          --conn-host '${NBA_DB_HOST:-host.docker.internal}' \
          --conn-schema 'nba_team' \
          --conn-login '${DB_USER:-mlb_user}' \
          --conn-password '${DB_PASSWORD:-${DB_PASSWORD}}' \
          --conn-port '5538'

        airflow connections add 'nba_intelligence_db' \
          --conn-type 'postgres' \
          --conn-host '${NBA_DB_HOST:-host.docker.internal}' \
          --conn-schema 'nba_intelligence' \
          --conn-login '${DB_USER:-mlb_user}' \
          --conn-password '${DB_PASSWORD:-${DB_PASSWORD}}' \
          --conn-port '5539'

        echo "Airflow initialization complete!"
    user: "0:0"
    restart: "no"
    networks:
      - airflow-network

  # --------------------------------------------------------------------------
  # Airflow CLI (for one-off commands)
  # --------------------------------------------------------------------------
  airflow-cli:
    <<: *airflow-common
    container_name: airflow-cli
    profiles:
      - cli
    command:
      - bash
      - -c
      - |
        # Keep container running for interactive use
        tail -f /dev/null
    networks:
      - airflow-network
      - nba-network

# ============================================================================
# Networks
# ============================================================================
networks:
  airflow-network:
    driver: bridge
  nba-network:
    # Connect to existing NBA database network if available
    external: true
    name: docker_default

# ============================================================================
# Volumes
# ============================================================================
volumes:
  airflow-postgres-data:
    driver: local
