{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Props Model Retraining Guide\n",
    "\n",
    "This notebook walks through the model retraining workflow:\n",
    "1. **Data Quality Checks** - Validate data before training\n",
    "2. **Build Training Dataset** - Extract features from historical props\n",
    "3. **Train Models** - Two-head stacked LightGBM\n",
    "4. **Evaluate Performance** - AUC, calibration, SHAP analysis\n",
    "5. **Deploy** - Move models to production\n",
    "\n",
    "**Current Production Models:** XL (102 features), trained Nov 6, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Project imports\n",
    "from nba.core.data_quality_checks import DataQualityChecker\n",
    "from nba.config.database import get_intelligence_db_config\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Quality Checks\n",
    "\n",
    "Always validate data before training. These checks catch:\n",
    "- Stale data (old game logs)\n",
    "- Missing coverage (players without rolling stats)\n",
    "- Null values in critical fields\n",
    "- Home/away imbalance (the bug that invalidated Nov 2025 models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pre-training checks\n",
    "checker = DataQualityChecker()\n",
    "checker.connect()\n",
    "\n",
    "# Run all checks\n",
    "success = checker.run_pre_training_checks()\n",
    "\n",
    "if not success:\n",
    "    print(\"\\n⚠️  Fix data issues before proceeding!\")\n",
    "else:\n",
    "    print(\"\\n✅ Ready to train\")\n",
    "\n",
    "checker.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Inspect Training Data\n",
    "\n",
    "Training datasets are pre-built in `nba/features/datasets/`. To rebuild:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing training data\n",
    "MARKET = 'POINTS'  # Change to REBOUNDS for other market\n",
    "\n",
    "df = pd.read_csv(f'../nba/features/datasets/xl_training_{MARKET}_2023_2025.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['game_date'].min()} to {df['game_date'].max()}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['hit_over'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check home/away balance (critical!)\n",
    "print(\"Home/Away Distribution:\")\n",
    "print(df['is_home'].value_counts(normalize=True))\n",
    "\n",
    "# Should be ~50/50. If 100% home, data is corrupted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature completeness\n",
    "null_pct = df.isnull().sum() / len(df) * 100\n",
    "high_null = null_pct[null_pct > 5].sort_values(ascending=False)\n",
    "\n",
    "if len(high_null) > 0:\n",
    "    print(\"Features with >5% null:\")\n",
    "    print(high_null)\n",
    "else:\n",
    "    print(\"✅ All features have <5% nulls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model\n",
    "\n",
    "The training script handles:\n",
    "- Temporal train/test split (70/30)\n",
    "- Two-head architecture (regressor + classifier)\n",
    "- Isotonic calibration\n",
    "- Blending (60% classifier, 40% residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Train via shell (recommended)\n",
    "!cd .. && make train-points\n",
    "\n",
    "# Option 2: Train directly\n",
    "# !python ../nba/models/train_market.py --market POINTS --data ../nba/features/datasets/xl_training_POINTS_2023_2025.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load model metadata\n",
    "metadata_path = Path(f'../nba/models/saved_xl/{MARKET.lower()}_xl_metadata.json')\n",
    "\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path) as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"Model: {MARKET}\")\n",
    "    print(f\"Trained: {metadata.get('trained_date', 'Unknown')}\")\n",
    "    print(f\"Features: {metadata.get('features', {}).get('count', 'Unknown')}\")\n",
    "    print(\"\\nRegressor Metrics:\")\n",
    "    for k, v in metadata.get('metrics', {}).get('regressor', {}).items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"\\nClassifier Metrics:\")\n",
    "    for k, v in metadata.get('metrics', {}).get('classifier', {}).items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    print(f\"No metadata found at {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP Feature Importance\n",
    "\n",
    "SHAP values show which features drive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display pre-generated SHAP plots\n",
    "shap_bar = Path(f'../nba/models/model_cards/images/{MARKET.lower()}_shap_bar.png')\n",
    "shap_beeswarm = Path(f'../nba/models/model_cards/images/{MARKET.lower()}_shap_beeswarm.png')\n",
    "\n",
    "if shap_bar.exists():\n",
    "    print(\"Top Features by Mean |SHAP|:\")\n",
    "    display(Image(filename=str(shap_bar), width=600))\n",
    "\n",
    "if shap_beeswarm.exists():\n",
    "    print(\"\\nFeature Impact Distribution:\")\n",
    "    display(Image(filename=str(shap_beeswarm), width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fresh SHAP analysis (takes ~2 min)\n",
    "# !python -m nba.models.generate_feature_importance --market POINTS --debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calibration Check\n",
    "\n",
    "Good calibration means predicted probabilities match actual hit rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test predictions (if saved during training)\n",
    "test_preds_path = Path(f'../nba/models/saved_xl/{MARKET.lower()}_test_predictions.csv')\n",
    "\n",
    "if test_preds_path.exists():\n",
    "    test_df = pd.read_csv(test_preds_path)\n",
    "    \n",
    "    # Calibration plot\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    \n",
    "    prob_true, prob_pred = calibration_curve(\n",
    "        test_df['actual'], test_df['predicted_prob'], n_bins=10\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "    plt.plot(prob_pred, prob_true, 's-', label='Model')\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.title(f'{MARKET} Calibration Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Test predictions not saved. Re-run training with --save-predictions flag.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare to Previous Model\n",
    "\n",
    "Before deploying, compare new model to current production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model registry\n",
    "import toml\n",
    "\n",
    "registry_path = Path('../nba/models/MODEL_REGISTRY.toml')\n",
    "if registry_path.exists():\n",
    "    registry = toml.load(registry_path)\n",
    "    \n",
    "    print(\"Model Registry:\")\n",
    "    for market, info in registry.get('models', {}).items():\n",
    "        print(f\"\\n{market}:\")\n",
    "        print(f\"  Version: {info.get('version', 'Unknown')}\")\n",
    "        print(f\"  AUC: {info.get('auc', 'Unknown')}\")\n",
    "        print(f\"  Status: {info.get('status', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploy New Model\n",
    "\n",
    "If the new model outperforms production:\n",
    "\n",
    "```bash\n",
    "# Models are automatically saved to nba/models/saved_xl/\n",
    "# Update MODEL_REGISTRY.toml with new metrics\n",
    "# Run validation on recent picks\n",
    "./nba/nba-predictions.sh validate --7d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "**Rebuild training data:**\n",
    "```bash\n",
    "make build-dataset\n",
    "```\n",
    "\n",
    "**Train all models:**\n",
    "```bash\n",
    "make train\n",
    "```\n",
    "\n",
    "**Check drift:**\n",
    "```bash\n",
    "python -m nba.core.cli_drift_check --market POINTS --check-latest\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
